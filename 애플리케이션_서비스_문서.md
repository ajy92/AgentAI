# LGM Project - AI 기반 대화형 에이전트 서비스

## 1. 서비스 개요

LGM Project는 LangGraph와 MCP(Model Context Protocol)를 활용하여 구축된 AI 기반 대화형 에이전트 서비스입니다. 이 서비스는 AWS의 완전관리형 RAG(Retrieval-Augmented Generation) 서비스인 Knowledge Base를 활용하여 문서 검색 및 분석을 수행하며, 다양한 MCP 서버를 통해 코드 실행, AWS 리소스 관리 등의 기능을 제공합니다.

### 1.1 주요 특징
- **LangGraph 기반 ReAct 에이전트**: 대화형 AI 에이전트로 복잡한 작업을 단계별로 처리
- **MCP 통합**: 다양한 도구와 서비스를 MCP 프로토콜을 통해 통합
- **AWS Knowledge Base 활용**: 완전관리형 RAG 서비스로 문서 검색 및 분석
- **멀티 모달 지원**: 텍스트, 이미지 처리 및 생성 기능
- **Streamlit 기반 UI**: 사용자 친화적인 웹 인터페이스

## 2. 아키텍처 설계

### 2.1 전체 아키텍처

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Streamlit UI  │    │   LangGraph     │    │   MCP Servers   │
│   (Frontend)    │◄──►│   Agent         │◄──►│   (Tools)       │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                                │
                                ▼
                       ┌─────────────────┐
                       │   AWS Services  │
                       │   - Bedrock     │
                       │   - Knowledge   │
                       │     Base        │
                       │   - S3          │
                       └─────────────────┘
```

### 2.2 핵심 컴포넌트

#### 2.2.1 LangGraph 에이전트 (`langgraph_agent.py`)
- **ReAct 패턴 구현**: Reasoning과 Acting을 반복하여 복잡한 작업 수행
- **상태 관리**: 대화 히스토리와 컨텍스트를 체계적으로 관리
- **도구 통합**: MCP를 통해 다양한 도구들을 동적으로 활용

#### 2.2.2 MCP 서버들
1. **kb-retriever** (`mcp_server_retrieve.py`): Knowledge Base 검색
2. **repl-coder** (`mcp_server_repl_coder.py`): 코드 실행 및 분석
3. **use-aws** (`mcp_server_use_aws.py`): AWS 리소스 관리

#### 2.2.3 채팅 시스템 (`chat.py`)
- **멀티 모델 지원**: Claude, Nova, OpenAI 모델 지원
- **이미지 처리**: 이미지 업로드 및 분석 기능
- **대화 관리**: 세션 기반 대화 히스토리 관리

## 3. 주요 기능

### 3.1 RAG 기반 문서 검색
- **Knowledge Base 활용**: AWS의 완전관리형 RAG 서비스
- **벡터 검색**: 의미 기반 문서 검색
- **다양한 문서 형식 지원**: PDF, TXT, CSV, PPT, DOC, 이미지 등

### 3.2 코드 실행 및 분석
- **Python 코드 실행**: 안전한 샌드박스 환경에서 코드 실행
- **데이터 시각화**: Matplotlib, Seaborn 등을 활용한 그래프 생성
- **파일 처리**: 다양한 파일 형식의 업로드 및 처리

### 3.3 AWS 리소스 관리
- **CLI 기반 관리**: AWS CLI 명령어를 MCP를 통해 실행
- **리소스 조회**: EC2, S3, Lambda 등 AWS 서비스 상태 확인
- **자동화**: 반복적인 AWS 작업 자동화

### 3.4 QA 에이전트
- **테스트 케이스 생성**: RAG 검색 결과를 바탕으로 테스트 케이스 자동 생성
- **문서 분석**: 업로드된 문서의 내용 분석 및 요약
- **질의응답**: 문서 기반 정확한 답변 제공

## 4. 기술 스택

### 4.1 백엔드
- **Python 3.13**: 메인 프로그래밍 언어
- **LangGraph**: AI 에이전트 오케스트레이션
- **LangChain**: LLM 통합 및 체인 구성
- **FastMCP**: MCP 서버 구현
- **Boto3**: AWS 서비스 통합

### 4.2 프론트엔드
- **Streamlit**: 웹 UI 프레임워크
- **Streamlit-Chat**: 채팅 인터페이스

### 4.3 AI/ML
- **AWS Bedrock**: LLM 서비스 (Claude, Nova, OpenAI)
- **AWS Knowledge Base**: RAG 서비스
- **OpenSearch**: 벡터 검색 엔진

### 4.4 인프라
- **AWS S3**: 문서 및 데이터 저장
- **AWS IAM**: 접근 권한 관리
- **Docker**: 컨테이너화 (선택사항)

## 5. 설치 및 설정

### 5.1 필수 요구사항
```bash
# Python 패키지 설치
pip install -r requirements.txt

# AWS CLI 설정 (선택사항)
aws configure
```

### 5.2 설정 파일 구성
```json
{
  "projectName": "mcp",
  "knowledge_base_id": "YOUR_KNOWLEDGE_BASE_ID",
  "region": "ap-northeast-2",
  "aws": {
    "aws_access_key_id": "YOUR_ACCESS_KEY",
    "aws_secret_access_key": "YOUR_SECRET_KEY"
  },
  "accountId": "YOUR_ACCOUNT_ID"
}
```

### 5.3 실행 방법
```bash
# Streamlit 애플리케이션 실행
streamlit run application/app.py
```

## 6. 사용 방법

### 6.1 기본 사용법
1. **에이전트 선택**: 왼쪽 메뉴에서 원하는 에이전트 선택
2. **MCP 서버 선택**: 사용할 도구들 선택
3. **질문 입력**: 텍스트 또는 이미지로 질문 입력
4. **결과 확인**: 에이전트의 응답 및 생성된 결과 확인

### 6.2 주요 사용 사례

#### 6.2.1 문서 검색 및 분석
```
사용자: "AWS Lambda에 대해 설명해주세요"
에이전트: Knowledge Base에서 관련 문서를 검색하여 상세한 설명 제공
```

#### 6.2.2 코드 실행 및 시각화
```
사용자: "DNA의 나선형 구조를 그려주세요"
에이전트: Python 코드를 생성하여 matplotlib으로 DNA 구조 시각화
```

#### 6.2.3 AWS 리소스 관리
```
사용자: "현재 실행 중인 EC2 인스턴스를 확인해주세요"
에이전트: AWS CLI를 통해 EC2 인스턴스 목록 조회 및 표시
```

## 7. 보안 및 권한 관리

### 7.1 AWS IAM 권한
- **Bedrock 접근**: AI 모델 사용을 위한 권한
- **Knowledge Base 접근**: RAG 서비스 사용을 위한 권한
- **S3 접근**: 문서 저장 및 검색을 위한 권한
- **기타 AWS 서비스**: 필요한 서비스별 최소 권한

### 7.2 코드 실행 보안
- **샌드박스 환경**: 격리된 환경에서 코드 실행
- **권한 제한**: 파일 시스템 접근 제한
- **실행 시간 제한**: 무한 루프 방지

## 8. 모니터링 및 로깅

### 8.1 로깅 시스템
- **구조화된 로깅**: JSON 형태의 로그 출력
- **레벨별 로깅**: DEBUG, INFO, WARNING, ERROR 레벨
- **컨텍스트 정보**: 요청 ID, 사용자 ID 등 포함

### 8.2 성능 모니터링
- **응답 시간 측정**: 각 단계별 처리 시간 측정
- **리소스 사용량**: 메모리, CPU 사용량 모니터링
- **에러 추적**: 예외 발생 시 상세한 스택 트레이스

## 9. 확장성 및 개선 방향

### 9.1 현재 제한사항
- **단일 인스턴스**: 수평 확장 제한
- **메모리 제한**: 대용량 문서 처리 시 메모리 부족 가능
- **동시 사용자**: 다중 사용자 동시 접근 제한

### 9.2 개선 계획
- **마이크로서비스 아키텍처**: 서비스별 독립적 배포
- **캐싱 시스템**: Redis를 활용한 응답 캐싱
- **로드 밸런싱**: 다중 인스턴스 지원
- **데이터베이스 통합**: PostgreSQL/MongoDB 연동

## 10. 문제 해결

### 10.1 일반적인 문제
- **Knowledge Base 연결 실패**: AWS 자격 증명 확인
- **MCP 서버 오류**: 서버 프로세스 상태 확인
- **메모리 부족**: 대용량 파일 처리 시 청크 단위 처리

### 10.2 디버깅 방법
- **로그 확인**: 상세한 로그를 통한 오류 추적
- **단계별 테스트**: 각 컴포넌트별 개별 테스트
- **AWS 콘솔 확인**: AWS 서비스 상태 확인

## 11. 라이선스 및 기여

### 11.1 라이선스
- **오픈소스**: MIT 라이선스
- **상업적 사용**: 자유로운 상업적 사용 가능

### 11.2 기여 방법
- **이슈 리포트**: 버그 리포트 및 기능 요청
- **풀 리퀘스트**: 코드 개선 및 새 기능 추가
- **문서화**: 사용법 및 API 문서 개선

## 12. 결론

LGM Project는 현대적인 AI 기술을 활용하여 복잡한 작업을 자동화하고 사용자에게 직관적인 인터페이스를 제공하는 혁신적인 서비스입니다. LangGraph와 MCP의 조합을 통해 확장 가능하고 유연한 AI 에이전트 시스템을 구축했으며, AWS의 관리형 서비스를 활용하여 안정적이고 신뢰할 수 있는 서비스를 제공합니다.

이 서비스는 기업의 문서 관리, 코드 개발, 인프라 관리 등 다양한 분야에서 활용될 수 있으며, 지속적인 개선을 통해 더욱 강력하고 유용한 도구로 발전할 것입니다.
